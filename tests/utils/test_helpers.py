#!/usr/bin/env python3
"""
ğŸ§ª SurveyXæµ‹è¯•è¾…åŠ©å‡½æ•°

æä¾›æµ‹è¯•è¿‡ç¨‹ä¸­å¸¸ç”¨çš„è¾…åŠ©åŠŸèƒ½

ä½œè€…: SurveyX Team
åˆ›å»ºæ—¶é—´: 2024-01
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = project_root / "outputs"
    output_dir.mkdir(exist_ok=True)

    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)

    return {
        "project_root": project_root,
        "output_dir": output_dir,
        "log_dir": log_dir
    }

def check_file_exists(file_path: str, description: str = "") -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    path = Path(file_path)
    exists = path.exists()

    status = "âœ… å­˜åœ¨" if exists else "âŒ ä¸å­˜åœ¨"
    desc = f" - {description}" if description else ""

    print(f"{status} {file_path}{desc}")

    return exists

def check_directory_contents(dir_path: str, expected_files: List[str] = None) -> Dict[str, Any]:
    """æ£€æŸ¥ç›®å½•å†…å®¹"""
    path = Path(dir_path)

    if not path.exists():
        return {"exists": False, "error": f"ç›®å½•ä¸å­˜åœ¨: {dir_path}"}

    contents = {
        "exists": True,
        "total_files": 0,
        "directories": [],
        "files": []
    }

    for item in path.iterdir():
        if item.is_file():
            contents["files"].append(item.name)
        elif item.is_dir():
            contents["directories"].append(item.name)
        contents["total_files"] += 1

    # æ£€æŸ¥é¢„æœŸæ–‡ä»¶
    if expected_files:
        missing_files = []
        for expected in expected_files:
            if expected not in contents["files"]:
                missing_files.append(expected)

        contents["missing_files"] = missing_files
        contents["expected_files_present"] = len(missing_files) == 0

    return contents

def measure_execution_time(func, *args, **kwargs) -> tuple:
    """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
    start_time = time.time()
    try:
        result = func(*args, **kwargs)
        success = True
        error = None
    except Exception as e:
        result = None
        success = False
        error = str(e)

    execution_time = time.time() - start_time

    return {
        "result": result,
        "success": success,
        "error": error,
        "execution_time": execution_time
    }

def validate_embedding_vector(embedding: List[float], expected_dim: int = 768) -> Dict[str, Any]:
    """éªŒè¯embeddingå‘é‡"""
    validation = {
        "is_list": isinstance(embedding, list),
        "length": len(embedding) if isinstance(embedding, list) else 0,
        "expected_dim": expected_dim,
        "dimension_correct": False,
        "value_range": {"min": 0, "max": 0},
        "all_finite": False
    }

    if not isinstance(embedding, list):
        return validation

    validation["dimension_correct"] = len(embedding) == expected_dim

    if embedding:
        import math
        validation["value_range"]["min"] = min(embedding)
        validation["value_range"]["max"] = max(embedding)
        validation["all_finite"] = all(math.isfinite(x) for x in embedding)

    return validation

def test_api_connectivity(url: str, headers: Dict[str, str] = None, timeout: int = 10) -> Dict[str, Any]:
    """æµ‹è¯•APIè¿æ¥æ€§"""
    import requests

    result = {
        "url": url,
        "reachable": False,
        "status_code": None,
        "response_time": 0,
        "error": None
    }

    try:
        start_time = time.time()
        response = requests.get(url, headers=headers, timeout=timeout)
        result["response_time"] = time.time() - start_time
        result["status_code"] = response.status_code
        result["reachable"] = True

    except requests.exceptions.Timeout:
        result["error"] = "è¿æ¥è¶…æ—¶"
    except requests.exceptions.ConnectionError:
        result["error"] = "è¿æ¥é”™è¯¯"
    except requests.exceptions.RequestException as e:
        result["error"] = f"è¯·æ±‚é”™è¯¯: {str(e)}"

    return result

def load_test_data(file_path: str) -> Optional[Any]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    path = Path(file_path)

    if not path.exists():
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None

    try:
        if file_path.endswith('.json'):
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif file_path.endswith('.txt'):
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        return None

def save_test_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # æ·»åŠ æ—¶é—´æˆ³
    results["timestamp"] = time.time()
    results["timestamp_str"] = time.strftime("%Y-%m-%d %H:%M:%S")

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

def print_test_summary(test_results: Dict[str, Any]):
    """æ‰“å°æµ‹è¯•æ€»ç»“"""
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*50)

    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result.get("success", False))

    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    success_rate = passed_tests / total_tests if total_tests > 0 else 0
    print(f"é€šè¿‡ç‡: {success_rate:.1%}")
    for test_name, result in test_results.items():
        status = "âœ…" if result.get("success", False) else "âŒ"
        exec_time = result.get("execution_time", 0)
        print(f"  {status} {test_name}: {exec_time:.3f}ç§’")
        if not result.get("success", False) and "error" in result:
            print(f"      é”™è¯¯: {result['error']}")

    print("="*50)

    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"âš ï¸ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")

def create_sample_markdown_content() -> str:
    """åˆ›å»ºç¤ºä¾‹Markdownå†…å®¹ç”¨äºæµ‹è¯•"""
    return """
# Sample Research Paper

## Abstract

This is a sample research paper for testing SurveyX embed model functionality.
It contains various sections and content types.

## Introduction

Large Language Models (LLMs) have revolutionized the field of natural language processing.
This paper explores the latest developments in this area.

## Methodology

### Data Collection

We collected data from multiple sources including academic papers and web content.

### Model Architecture

The model uses transformer architecture with attention mechanisms.

## Results

Our experiments show significant improvements over baseline methods.

### Performance Metrics

- Accuracy: 95.2%
- F1-Score: 94.8%
- Precision: 96.1%

## Conclusion

This study demonstrates the effectiveness of our approach for various NLP tasks.

## References

[1] Smith, J. et al. "Advances in NLP". Journal of AI, 2023.
[2] Johnson, A. "Transformer Models". Conference on ML, 2024.
"""

def benchmark_embedding_speed(embed_agent, texts: List[str], num_runs: int = 3) -> Dict[str, Any]:
    """åŸºå‡†æµ‹è¯•embeddingé€Ÿåº¦"""
    import statistics

    results = {
        "num_texts": len(texts),
        "num_runs": num_runs,
        "times": [],
        "avg_time": 0,
        "min_time": 0,
        "max_time": 0,
        "texts_per_second": 0
    }

    for i in range(num_runs):
        start_time = time.time()
        embeddings = embed_agent.batch_local_embed(texts)
        end_time = time.time()

        run_time = end_time - start_time
        results["times"].append(run_time)

    if results["times"]:
        results["avg_time"] = statistics.mean(results["times"])
        results["min_time"] = min(results["times"])
        results["max_time"] = max(results["times"])
        results["texts_per_second"] = len(texts) / results["avg_time"]

    return results

if __name__ == "__main__":
    print("ğŸ§ª SurveyXæµ‹è¯•è¾…åŠ©å‡½æ•°")
    print("è¿™äº›æ˜¯æµ‹è¯•å·¥å…·å‡½æ•°ï¼Œè¯·é€šè¿‡å…·ä½“çš„æµ‹è¯•æ–‡ä»¶è°ƒç”¨ã€‚")

    # æ¼”ç¤ºåŸºæœ¬åŠŸèƒ½
    env = setup_test_environment()
    print(f"é¡¹ç›®æ ¹ç›®å½•: {env['project_root']}")
    print(f"è¾“å‡ºç›®å½•: {env['output_dir']}")

    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    check_file_exists("src/configs/config.py", "ä¸»é…ç½®æ–‡ä»¶")
    check_file_exists("requirements.txt", "ä¾èµ–æ–‡ä»¶")
    check_file_exists("models/bge-base/config.json", "Embedæ¨¡å‹é…ç½®")
