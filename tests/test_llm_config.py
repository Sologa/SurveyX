#!/usr/bin/env python3
"""
ğŸ§ª SurveyX LLMé…ç½®æµ‹è¯•è„šæœ¬

éªŒè¯LLMç›¸å…³çš„é…ç½®æ˜¯å¦æ­£ç¡®

ä½œè€…: SurveyX Team
åˆ›å»ºæ—¶é—´: 2024-01
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.configs.logger import get_logger
from src.configs.config import (
    REMOTE_URL,
    TOKEN,
    DEFAULT_CHATAGENT_MODEL,
    ADVANCED_CHATAGENT_MODEL,
    DEFAULT_EMBED_ONLINE_MODEL,
    EMBED_REMOTE_URL,
    EMBED_TOKEN
)

logger = get_logger("test_llm_config")

class LLMConfigTester:
    """LLMé…ç½®æµ‹è¯•ç±»"""

    def __init__(self):
        self.config_status = {}

    def test_config_file_exists(self):
        """æµ‹è¯•é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        logger.info("ğŸ”„ æ£€æŸ¥é…ç½®æ–‡ä»¶...")

        config_path = project_root / "src" / "configs" / "config.py"
        if config_path.exists():
            logger.info(f"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {config_path}")
            self.config_status["config_file"] = True
            return True
        else:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            self.config_status["config_file"] = False
            return False

    def test_api_configurations(self):
        """æµ‹è¯•APIé…ç½®"""
        logger.info("ğŸ”„ æ£€æŸ¥APIé…ç½®...")

        configs = {
            "REMOTE_URL": REMOTE_URL,
            "TOKEN": TOKEN,
            "DEFAULT_CHATAGENT_MODEL": DEFAULT_CHATAGENT_MODEL,
            "ADVANCED_CHATAGENT_MODEL": ADVANCED_CHATAGENT_MODEL,
            "DEFAULT_EMBED_ONLINE_MODEL": DEFAULT_EMBED_ONLINE_MODEL,
            "EMBED_REMOTE_URL": EMBED_REMOTE_URL,
            "EMBED_TOKEN": EMBED_TOKEN
        }

        all_valid = True

        for key, value in configs.items():
            if value and value != f"your {key.lower()} here":
                logger.info(f"âœ… {key}: å·²é…ç½®")
                self.config_status[key] = True
            else:
                logger.warning(f"âš ï¸ {key}: æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
                self.config_status[key] = False
                if key in ["TOKEN", "EMBED_TOKEN"]:
                    all_valid = False

        return all_valid

    def test_model_paths(self):
        """æµ‹è¯•æ¨¡å‹è·¯å¾„"""
        logger.info("ğŸ”„ æ£€æŸ¥æ¨¡å‹è·¯å¾„...")

        # æ£€æŸ¥embedæ¨¡å‹è·¯å¾„
        embed_model_path = DEFAULT_EMBED_ONLINE_MODEL

        if embed_model_path.startswith("./") or embed_model_path.startswith("/"):
            # æœ¬åœ°è·¯å¾„
            full_path = project_root / embed_model_path.lstrip("./")
            if full_path.exists():
                logger.info(f"âœ… Embedæ¨¡å‹è·¯å¾„å­˜åœ¨: {full_path}")
                self.config_status["embed_model_path"] = True

                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                model_files = [
                    "config.json",
                    "pytorch_model.bin",
                    "tokenizer_config.json",
                    "vocab.txt"
                ]

                missing_files = []
                for file in model_files:
                    if not (full_path / file).exists():
                        missing_files.append(file)

                if not missing_files:
                    logger.info("âœ… Embedæ¨¡å‹æ–‡ä»¶å®Œæ•´")
                else:
                    logger.warning(f"âš ï¸ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing_files}")
                    self.config_status["embed_model_complete"] = False
                return True
            else:
                logger.error(f"âŒ Embedæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {full_path}")
                logger.error("ğŸ’¡ è¯·ä¸‹è½½æ¨¡å‹: huggingface-cli download BAAI/bge-base-en-v1.5 --local-dir ./models/bge-base")
                self.config_status["embed_model_path"] = False
                return False
        else:
            # HuggingFaceæ¨¡å‹å
            logger.info(f"â„¹ï¸ ä½¿ç”¨HuggingFaceæ¨¡å‹: {embed_model_path}")
            self.config_status["embed_model_hf"] = True
            return True

    def test_network_connectivity(self):
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        logger.info("ğŸ”„ æµ‹è¯•ç½‘ç»œè¿æ¥...")

        import requests

        # æµ‹è¯•OpenAI APIè¿æ¥
        try:
            # åªæ˜¯æµ‹è¯•è¿æ¥ï¼Œä¸å‘é€å®é™…è¯·æ±‚
            response = requests.get("https://api.openai.com/v1/models",
                                  headers={"Authorization": f"Bearer {TOKEN}"},
                                  timeout=10)
            if response.status_code == 401:
                logger.info("âœ… OpenAI APIè¿æ¥æ­£å¸¸ (401è¡¨ç¤ºè®¤è¯é—®é¢˜ï¼Œä½†è¿æ¥æˆåŠŸ)")
                self.config_status["openai_api"] = True
            elif response.status_code == 200:
                logger.info("âœ… OpenAI APIè¿æ¥æ­£å¸¸")
                self.config_status["openai_api"] = True
            else:
                logger.warning(f"âš ï¸ OpenAI APIå“åº”å¼‚å¸¸: {response.status_code}")
                self.config_status["openai_api"] = False
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ OpenAI APIè¿æ¥å¤±è´¥: {e}")
            self.config_status["openai_api"] = False

        # æµ‹è¯•embed APIè¿æ¥ (å¦‚æœä½¿ç”¨è¿œç¨‹embed)
        if EMBED_REMOTE_URL != "https://api.siliconflow.cn/v1/embeddings":
            try:
                response = requests.get(EMBED_REMOTE_URL, timeout=5)
                logger.info(f"âœ… Embed APIè¿æ¥æ­£å¸¸: {EMBED_REMOTE_URL}")
                self.config_status["embed_api"] = True
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ Embed APIè¿æ¥é—®é¢˜: {e}")
                self.config_status["embed_api"] = False

        return self.config_status.get("openai_api", False)

    def test_dependencies(self):
        """æµ‹è¯•ä¾èµ–åŒ…"""
        logger.info("ğŸ”„ æ£€æŸ¥Pythonä¾èµ–...")

        required_packages = [
            "openai",
            "llama_index",
            "transformers",
            "torch",
            "numpy",
            "tqdm"
        ]

        missing_packages = []
        for package in required_packages:
            try:
                __import__(package.replace("-", "_"))
                logger.info(f"âœ… {package}: å·²å®‰è£…")
            except ImportError:
                logger.error(f"âŒ {package}: æœªå®‰è£…")
                missing_packages.append(package)

        if not missing_packages:
            logger.info("âœ… æ‰€æœ‰å¿…éœ€ä¾èµ–å·²å®‰è£…")
            self.config_status["dependencies"] = True
            return True
        else:
            logger.error(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {missing_packages}")
            logger.error("ğŸ’¡ è¯·è¿è¡Œ: pip install -r requirements.txt")
            self.config_status["dependencies"] = False
            return False

    def generate_config_report(self):
        """ç”Ÿæˆé…ç½®æŠ¥å‘Š"""
        logger.info("ğŸ”„ ç”Ÿæˆé…ç½®æŠ¥å‘Š...")

        report = {
            "timestamp": str(Path(__file__).stat().st_mtime),
            "config_status": self.config_status,
            "system_info": {
                "python_version": sys.version,
                "working_directory": str(project_root),
                "platform": sys.platform
            },
            "recommendations": []
        }

        # ç”Ÿæˆå»ºè®®
        if not self.config_status.get("config_file", False):
            report["recommendations"].append("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥src/configs/config.py")

        if not self.config_status.get("TOKEN", False):
            report["recommendations"].append("OpenAI API Tokenæœªè®¾ç½®ï¼Œè¯·åœ¨config.pyä¸­é…ç½®TOKEN")

        if not self.config_status.get("embed_model_path", False) and not self.config_status.get("embed_model_hf", False):
            report["recommendations"].append("Embedæ¨¡å‹è·¯å¾„æœªé…ç½®ï¼Œè¯·è®¾ç½®DEFAULT_EMBED_ONLINE_MODEL")

        if not self.config_status.get("dependencies", False):
            report["recommendations"].append("Pythonä¾èµ–ä¸å®Œæ•´ï¼Œè¯·è¿è¡Œ: pip install -r requirements.txt")

        if not self.config_status.get("openai_api", False):
            report["recommendations"].append("OpenAI APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’ŒAPIå¯†é’¥")

        return report

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹LLMé…ç½®å…¨é¢æµ‹è¯•")
        logger.info("=" * 50)

        test_results = []

        # 1. é…ç½®æ–‡ä»¶æµ‹è¯•
        test_results.append(("é…ç½®æ–‡ä»¶", self.test_config_file_exists()))

        # 2. APIé…ç½®æµ‹è¯•
        test_results.append(("APIé…ç½®", self.test_api_configurations()))

        # 3. æ¨¡å‹è·¯å¾„æµ‹è¯•
        test_results.append(("æ¨¡å‹è·¯å¾„", self.test_model_paths()))

        # 4. ç½‘ç»œè¿æ¥æµ‹è¯•
        test_results.append(("ç½‘ç»œè¿æ¥", self.test_network_connectivity()))

        # 5. ä¾èµ–æµ‹è¯•
        test_results.append(("Pythonä¾èµ–", self.test_dependencies()))

        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        logger.info("=" * 50)
        logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")

        passed = 0
        total = len(test_results)

        for test_name, result in test_results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"  {test_name}: {status}")
            if result:
                passed += 1

        logger.info("-" * 30)
        success_rate = passed / total if total > 0 else 0
        logger.info(f"âœ… é€šè¿‡ç‡: {success_rate:.1%}")
        # ç”Ÿæˆé…ç½®æŠ¥å‘Š
        report = self.generate_config_report()

        logger.info("ğŸ“‹ é…ç½®å»ºè®®:")
        for i, rec in enumerate(report["recommendations"], 1):
            logger.info(f"  {i}. {rec}")

        if passed == total:
            logger.info("ğŸ‰ æ‰€æœ‰é…ç½®æµ‹è¯•é€šè¿‡ï¼æ‚¨çš„LLMé…ç½®æ­£ç¡®ã€‚")
        else:
            logger.warning(f"âš ï¸ {total - passed}ä¸ªé…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ ¹æ®å»ºè®®è¿›è¡Œä¿®å¤ã€‚")

        return test_results, report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª SurveyX LLMé…ç½®æµ‹è¯•å·¥å…·")
    print("=" * 40)

    # æ£€æŸ¥ç¯å¢ƒ
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ Pythonç‰ˆæœ¬: {sys.version}")
    print(f"ğŸ–¥ï¸ æ“ä½œç³»ç»Ÿ: {sys.platform}")

    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    key_files = [
        "src/configs/config.py",
        "requirements.txt",
        "models/bge-base/config.json"  # å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹
    ]

    print("\nğŸ“ å…³é”®æ–‡ä»¶çŠ¶æ€:")
    for file_path in key_files:
        full_path = project_root / file_path
        if full_path.exists():
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path} (ä¸å­˜åœ¨)")

    print()

    # è¿è¡Œæµ‹è¯•
    tester = LLMConfigTester()
    results, report = tester.run_all_tests()

    # ä¿å­˜æŠ¥å‘Š
    report_path = project_root / "tests" / "config_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ é…ç½®æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # è¿”å›é€€å‡ºç 
    passed = sum(1 for _, result in results if result)
    total = len(results)
    success_rate = passed / total if total > 0 else 0

    if success_rate == 1.0:
        print("\nğŸ‰ æ‰€æœ‰é…ç½®æµ‹è¯•é€šè¿‡ï¼æ‚¨çš„SurveyXç¯å¢ƒå·²å‡†å¤‡å°±ç»ªã€‚")
        return 0
    elif success_rate >= 0.5:
        print(f"\nâš ï¸ å¤§éƒ¨åˆ†é…ç½®æ­£ç¡® ({passed}/{total})ï¼Œä½†éœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ã€‚")
        return 1
    else:
        print(f"\nâŒ é…ç½®å­˜åœ¨ä¸¥é‡é—®é¢˜ ({passed}/{total})ï¼Œè¯·ä»”ç»†æ£€æŸ¥ã€‚")
        return 2

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
