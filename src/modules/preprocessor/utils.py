import argparse
import json
import re
import time
from datetime import datetime
from pathlib import Path

from src.configs.constants import BASE_DIR, OUTPUT_DIR
from src.configs.logger import get_logger
from src.models.LLM import ChatAgent
from src.configs.config import ADVANCED_CHATAGENT_MODEL
from src.models.LLM.utils import load_prompt
from src.modules.utils import save_result, update_config
from src.configs.config import DEFAULT_DATA_FETCHER_ENABLE_CACHE

logger = get_logger("src.modules.preprocessor.utils")


class ArgsNamespace(argparse.Namespace):
    title: str
    key_words: str
    page: int
    time_s: str
    time_e: str
    enable_cache: bool


def parse_arguments_for_preprocessor() -> ArgsNamespace:
    parser = argparse.ArgumentParser(description="Fetch data and Clean them.")
    parser.add_argument(
        "--title",
        type=str,
        default="Attention Heads of Large Language Models: A Survey",
        help="Input the title to generate survey.",
    )
    parser.add_argument(
        "--key_words",
        type=str,
        default="",
        help="Input the key_words to search on databases.",
    )
    parser.add_argument(
        "--page",
        type=str,
        default="5",
        help="Number of pages to crawl on Google Scholar.",
    )
    parser.add_argument(
        "--time_s",
        type=str,
        default="2017",
        help="Start year for filtering search results.",
    )
    parser.add_argument(
        "--time_e",
        type=str,
        default="2024",
        help="End year for filtering search results.",
    )
    parser.add_argument(
        "--enable_cache",
        type=bool,
        default=DEFAULT_DATA_FETCHER_ENABLE_CACHE,
        help="Whether import cache for preprocessing.",
    )
    return parser.parse_args()


def parse_arguments_for_integration_test() -> str:
    parser = argparse.ArgumentParser(description="Give the --task_id parameter.")
    parser.add_argument("--task_id", type=str, required=True, help="The ID of the task")
    args = parser.parse_args()
    return args.task_id


def parse_arguments_for_offline() -> ArgsNamespace:
    parser = argparse.ArgumentParser(description="Fetch data and Clean them.")
    parser.add_argument(
        "--title",
        type=str,
        default="Attention Heads of Large Language Models: A Survey",
        help="Input the title to generate survey.",
    )
    parser.add_argument(
        "--key_words",
        type=str,
        default="",
        help="Input the key_words to search on databases.",
    )
    parser.add_argument(
        "--ref_path",
        type=str,
        default="",
        help="Path to the reference papers for offline run.",
    )
    parser.add_argument(
        "--curated_json",
        type=str,
        default="",
        help=(
            "Optional: Path to curated metadata JSON (e.g., resources/offline_refs/included_papers_*.json). "
            "When provided, title/abstract/arxiv_id will be merged in and take precedence over MD extraction."
        ),
    )
    parser.add_argument(
        "--skip_llm",
        action="store_true",
        help="Skip ChatAgent steps (paper type classification and attribute extraction).",
    )
    parser.add_argument(
        "--only_clean",
        action="store_true",
        help="Only run the cleaning stage (produce papers/ and references.bib), then exit.",
    )

    return parser.parse_args()


def create_tmp_config(title: str, key_word: str):
    tmp_config = {}
    tmp_config["title"] = title
    tmp_config["key_words"] = gen_keyword(title, key_word)
    tmp_config["topic"] = gen_topic(title, tmp_config["key_words"])
    task_id = datetime.now().strftime("%Y-%m-%d-%H%M_") + tmp_config["key_words"][
        :5
    ].replace(" ", "_")
    tmp_config["task_id"] = task_id

    update_config(tmp_config, Path(OUTPUT_DIR) / task_id / "tmp_config.json")
    logger.info(f"Created tmp_config: {json.dumps(tmp_config, indent=4)}")
    return tmp_config


def gen_keyword(title: str, key_words: str) -> str:
    if len(key_words.split(",")) >= 6:
        return key_words

    chat_agent = ChatAgent()
    prompt = load_prompt(
        Path(f"{BASE_DIR}/resources/LLM/prompts/preprocessor/generate_keyword.md"),
        title=title,
        key_words=key_words,
    )

    res = chat_agent.remote_chat(prompt, model=ADVANCED_CHATAGENT_MODEL)
    new_keywords = re.findall(r"<Answer>(.*?)</Answer>", res)[0]
    if key_words:
        final_keywords = ",".join(
            key_words.split(",") + new_keywords.split(",")[:3]
        )  # only select first 3 generated keyword, to avoid misunderstanding
    else:
        final_keywords = new_keywords
    logger.info(f"Keywords: {final_keywords}")
    return final_keywords


def gen_topic(title: str, key_word: str) -> str:
    """Generate a detail description of the keyword in one sentence.
    This description is used to provide more infos about keyword.
    """
    chat = ChatAgent()
    prompt = load_prompt(
        Path(f"{BASE_DIR}/resources/LLM/prompts/preprocessor/generate_topic.md"),
        title=title,
        key_word=key_word,
    )
    topic = chat.remote_chat(prompt, model=ADVANCED_CHATAGENT_MODEL)
    return topic


def wait_for_crawling(seconds: int):
    """Sleep system for seconds."""
    for i in range(seconds):
        print(
            f"\rWaiting for crawling... remaining {seconds - i} seconds.   ",
            end="",
            flush=True,
        )
        time.sleep(1)
    print()


def save_papers(papers: list[dict], dir_path: Path):
    for paper in papers:
        p = Path(dir_path) / f"{paper.get('_id', paper['title'])}.json"
        save_result(json.dumps(paper, indent=4), p)


def get_tmp_config(task_id: str):
    tmp_config_path = Path(OUTPUT_DIR) / task_id / "tmp_config.json"
    with open(tmp_config_path, "r", encoding="utf-8") as file:
        tmp_config = json.load(file)
    return tmp_config
